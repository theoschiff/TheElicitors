{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3c247cd",
   "metadata": {},
   "source": [
    "## This is for creating and splitting the Poetry dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0951de28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c077f7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"DanFosing/public-domain-poetry\")\n",
    "dataset_df = pd.DataFrame(dataset[\"train\"])\n",
    "dataset_df = dataset_df[[\"Author\", \"Title\", \"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e74af33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df.rename(columns={\"text\":\"Text\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca82034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_poem_lines(poem, ratio=1/3):\n",
    "    lines = poem.splitlines()\n",
    "    split_idx = max(1, int(len(lines) * ratio))\n",
    "    start = \"\\n\".join(lines[:split_idx])\n",
    "    end = \"\\n\".join(lines[split_idx:])\n",
    "    return pd.Series([start, end])\n",
    "\n",
    "dataset_df[[\"poem_start\", \"poem_end\"]] = dataset_df[\"text\"].apply(split_poem_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80658146",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df.drop(columns=[\"text\"], inplace=True)\n",
    "dataset_df.rename(columns={\"Author\": \"author\", \"Title\": \"title\"}, inplace=True)\n",
    "dataset = Dataset.from_pandas(dataset_df)\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d483499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "dataset.push_to_hub(\"schifferlearning/Poetry-Categorized\", private=False, token=os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "906a5f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import cmudict\n",
    "\n",
    "# Ensure cmudict is available\n",
    "try:\n",
    "    nltk.data.find(\"corpora/cmudict\")\n",
    "except LookupError:\n",
    "    nltk.download(\"cmudict\")\n",
    "\n",
    "d = cmudict.dict()\n",
    "\n",
    "def count_syllables(word):\n",
    "    \"\"\"Count syllables in a word using cmudict or fallback to simple rule.\"\"\"\n",
    "    word = word.lower()\n",
    "    if word in d:\n",
    "        return max(len([y for y in pron if y[-1].isdigit()]) for pron in d[word])\n",
    "    return len(re.findall(r'[aeiouy]+', word.lower()))\n",
    "\n",
    "def total_syllables(line):\n",
    "    \"\"\"Count total syllables in a line.\"\"\"\n",
    "    words = re.findall(r'\\b\\w+\\b', line.lower())\n",
    "    return sum(count_syllables(w) for w in words)\n",
    "\n",
    "def classify_form(poem):\n",
    "    \"\"\"Classify a poem's form based on line count and syllable patterns.\"\"\"\n",
    "    lines = [line.strip() for line in poem.strip().splitlines() if line.strip()]\n",
    "    line_count = len(lines)\n",
    "    syllable_counts = [total_syllables(line) for line in lines]\n",
    "\n",
    "    # Haiku: 3 lines with 5-7-5 syllable pattern\n",
    "    if line_count == 3 and syllable_counts == [5, 7, 5]:\n",
    "        return \"haiku\"\n",
    "\n",
    "    # Tanka: 5 lines with 5-7-5-7-7 syllable pattern\n",
    "    if line_count == 5 and syllable_counts == [5, 7, 5, 7, 7]:\n",
    "        return \"tanka\"\n",
    "\n",
    "    # Limerick: 5 lines with approximate syllable counts\n",
    "    if line_count == 5 and all(8 <= s <= 9 for s in syllable_counts[:2]) and all(5 <= s <= 6 for s in syllable_counts[2:4]) and 8 <= syllable_counts[4] <= 9:\n",
    "        return \"limerick\"\n",
    "\n",
    "    # Sonnet: 14 lines\n",
    "    if line_count == 14:\n",
    "        return \"sonnet\"\n",
    "\n",
    "    # Quatrain: 4 lines with similar syllable counts\n",
    "    if line_count == 4 and max(syllable_counts) - min(syllable_counts) <= 2:\n",
    "        return \"quatrain\"\n",
    "\n",
    "    # Cinquain: 5 lines with specific syllable counts\n",
    "    if line_count == 5 and syllable_counts == [2, 4, 6, 8, 2]:\n",
    "        return \"cinquain\"\n",
    "\n",
    "    # Octave: 8 lines with similar syllable counts\n",
    "    if line_count == 8 and max(syllable_counts) - min(syllable_counts) <= 2:\n",
    "        return \"octave\"\n",
    "\n",
    "    # Decastich: 10 lines with similar syllable counts\n",
    "    if line_count == 10 and max(syllable_counts) - min(syllable_counts) <= 2:\n",
    "        return \"decastich\"\n",
    "\n",
    "    # Sestet: 6 lines with similar syllable counts\n",
    "    if line_count == 6 and max(syllable_counts) - min(syllable_counts) <= 2:\n",
    "        return \"sestet\"\n",
    "\n",
    "    # Couplet: 2 lines with similar syllable counts\n",
    "    if line_count == 2 and abs(syllable_counts[0] - syllable_counts[1]) <= 2:\n",
    "        return \"couplet\"\n",
    "\n",
    "    return \"free_verse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e38ed120",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df['Form'] = dataset_df['Text'].apply(classify_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "30c005ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "free_verse    32022\n",
       "sonnet         3484\n",
       "quatrain        893\n",
       "octave          813\n",
       "couplet         604\n",
       "sestet          352\n",
       "decastich       209\n",
       "limerick        122\n",
       "Name: Form, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df[\"Form\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ffe42c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "free_verse    800\n",
       "octave        800\n",
       "quatrain      800\n",
       "sonnet        800\n",
       "Name: Form, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "form_counts = dataset_df['Form'].value_counts()\n",
    "eligible_forms = form_counts[form_counts >= 800].index\n",
    "\n",
    "# filter the DataFrame to include only eligible forms\n",
    "eligible_df = dataset_df[dataset_df['Form'].isin(eligible_forms)]\n",
    "\n",
    "# sample 800 entries from each eligible form\n",
    "sampled_df = eligible_df.groupby('Form', group_keys=False).apply(lambda x: x.sample(n=800, random_state=42))\n",
    "sampled_df = sampled_df.reset_index(drop=True)\n",
    "\n",
    "sampled_df['Form'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "375ea28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625c9a42250e4db681639974b0a004ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8796f390e04244b4a25bce700db08c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ffb7c8111a5446ead2ddea86739bb1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e8605c61e240788a47499b6761eaea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/schifferlearning/Poetry-Categorized/commit/641f49bafd5602fe51d3cc0eab8638781b63d4c9', commit_message='Upload dataset', commit_description='', oid='641f49bafd5602fe51d3cc0eab8638781b63d4c9', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/schifferlearning/Poetry-Categorized', endpoint='https://huggingface.co', repo_type='dataset', repo_id='schifferlearning/Poetry-Categorized'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "dataset = Dataset.from_pandas(sampled_df)\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "dataset.push_to_hub(\"schifferlearning/Poetry-Categorized\", private=False, token=os.getenv(\"HUGGINGFACE_HUB_TOKEN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0ebd24",
   "metadata": {},
   "source": [
    "## This is for creating the math Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb53e267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "from fractions import Fraction\n",
    "\n",
    "dataset = load_dataset(\"Jiayi-Pan/Countdown-Tasks-3to4\")\n",
    "dataset = dataset[\"train\"].shuffle(seed=42).select(range(11000))\n",
    "dataset = dataset.train_test_split(test_size=1000, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d81d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_countdown_expression(nums, target):\n",
    "    \"\"\"\n",
    "    Find an arithmetic expression using each number in nums exactly once that evaluates to target.\n",
    "    Operations allowed: +, -, *, /, with parentheses.\n",
    "    Uses Fraction for exact arithmetic to avoid floating-point issues.\n",
    "\n",
    "    Args:\n",
    "        nums (list of int or float): input numbers\n",
    "        target (int or float): desired target value\n",
    "\n",
    "    Returns:\n",
    "        str: a string representation of the expression that evaluates to target\n",
    "\n",
    "    Raises:\n",
    "        ValueError: if no expression can be found\n",
    "    \"\"\"\n",
    "    # Prepare initial list of (value, expression) pairs\n",
    "    initial = [(Fraction(n), str(n)) for n in nums]\n",
    "    target_frac = Fraction(target)\n",
    "\n",
    "    def helper(pairs):\n",
    "        # If only one value remains, check if it equals the target\n",
    "        if len(pairs) == 1:\n",
    "            val, expr = pairs[0]\n",
    "            if val == target_frac:\n",
    "                return expr\n",
    "            return None\n",
    "\n",
    "        # Try all pairs of numbers\n",
    "        for i in range(len(pairs)):\n",
    "            for j in range(i + 1, len(pairs)):\n",
    "                a_val, a_expr = pairs[i]\n",
    "                b_val, b_expr = pairs[j]\n",
    "\n",
    "                # Build a new list of remaining numbers\n",
    "                rest = [pairs[k] for k in range(len(pairs)) if k not in (i, j)]\n",
    "\n",
    "                # Generate all possible operations\n",
    "                operations = [\n",
    "                    (a_val + b_val, f\"({a_expr}+{b_expr})\"),\n",
    "                    (a_val - b_val, f\"({a_expr}-{b_expr})\"),\n",
    "                    (b_val - a_val, f\"({b_expr}-{a_expr})\"),\n",
    "                    (a_val * b_val, f\"({a_expr}*{b_expr})\"),\n",
    "                ]\n",
    "\n",
    "                # Division operations, avoid division by zero\n",
    "                if b_val != 0:\n",
    "                    operations.append((a_val / b_val, f\"({a_expr}/{b_expr})\"))\n",
    "                if a_val != 0:\n",
    "                    operations.append((b_val / a_val, f\"({b_expr}/{a_expr})\"))\n",
    "\n",
    "                # Recurse on each possibility\n",
    "                for new_val, new_expr in operations:\n",
    "                    result = helper(rest + [(new_val, new_expr)])\n",
    "                    if result:\n",
    "                        val = eval(result, {\"__builtins__\": None}, {})\n",
    "                        assert abs(float(val) - float(target)) < 1e-5, f\"Invalid expression: {result}\"\n",
    "                        return result\n",
    "        return None\n",
    "\n",
    "    expression = helper(initial)\n",
    "    if expression is None:\n",
    "        raise ValueError(f\"No solution found for nums={nums} target={target}\")\n",
    "    return expression\n",
    "\n",
    "def add_gold_answer_to_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Load the HF dataset, compute the countdown solution for each example, and return a new Dataset with 'gold_answer'.\n",
    "\n",
    "    Args:\n",
    "        dataset : Hugging Face dataset\n",
    "\n",
    "    Returns:\n",
    "        datasets.Dataset: with additional 'gold_answer' column\n",
    "    \"\"\"\n",
    "    def solve(example):\n",
    "        nums = example['nums']\n",
    "        target = example['target']\n",
    "        try:\n",
    "            example['gold_answer'] = find_countdown_expression(nums, target)\n",
    "        except ValueError:\n",
    "            example['gold_answer'] = None\n",
    "        return example\n",
    "\n",
    "    new_ds = dataset.map(solve)\n",
    "    return new_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74890a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"] = add_gold_answer_to_dataset(dataset[\"train\"])\n",
    "dataset[\"test\"] = add_gold_answer_to_dataset(dataset[\"test\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0e5337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "dataset.push_to_hub(\"Jeremmmyyyyy/Math\", private=False, token=os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0803c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
